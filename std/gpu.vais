# GPU Runtime Module for Vais
# Provides GPU kernel definition and execution support
#
# Two categories of functions:
# 1. Kernel-side intrinsics (thread_idx, sync_threads, atomics, etc.)
#    - These are replaced by the GPU codegen backend when compiling with --gpu
#    - Host-side placeholders return dummy values for type checking
# 2. Host-side runtime API (gpu_alloc, gpu_launch_kernel, etc.)
#    - These are extern functions linked from gpu_runtime.c
#    - Use with: vaisc build file.vais --gpu cuda --gpu-compile
#
# Usage:
#   U std/gpu
#
#   #[gpu]
#   F vector_add(a: *f64, b: *f64, c: *f64, n: i64) -> i64 {
#       idx := thread_idx_x() + block_idx_x() * block_dim_x()
#       I idx < n {
#           c[idx] = a[idx] + b[idx]
#       }
#       0
#   }

# ============================================================================
# Thread Indexing Functions
# ============================================================================

# Get thread index within block (x dimension)
# GPU intrinsic: replaced by threadIdx.x (CUDA) / get_local_id(0) (OpenCL)
F thread_idx_x() -> i64 = 0

# Get thread index within block (y dimension)
# GPU intrinsic: replaced by threadIdx.y (CUDA) / get_local_id(1) (OpenCL)
F thread_idx_y() -> i64 = 0

# Get thread index within block (z dimension)
# GPU intrinsic: replaced by threadIdx.z (CUDA) / get_local_id(2) (OpenCL)
F thread_idx_z() -> i64 = 0

# Get block index within grid (x dimension)
# GPU intrinsic: replaced by blockIdx.x (CUDA) / get_group_id(0) (OpenCL)
F block_idx_x() -> i64 = 0

# Get block index within grid (y dimension)
# GPU intrinsic: replaced by blockIdx.y (CUDA) / get_group_id(1) (OpenCL)
F block_idx_y() -> i64 = 0

# Get block index within grid (z dimension)
# GPU intrinsic: replaced by blockIdx.z (CUDA) / get_group_id(2) (OpenCL)
F block_idx_z() -> i64 = 0

# Get block dimension (threads per block, x dimension)
F block_dim_x() -> i64 = 256

# Get block dimension (threads per block, y dimension)
F block_dim_y() -> i64 = 1

# Get block dimension (threads per block, z dimension)
F block_dim_z() -> i64 = 1

# Get grid dimension (blocks per grid, x dimension)
F grid_dim_x() -> i64 = 1

# Get grid dimension (blocks per grid, y dimension)
F grid_dim_y() -> i64 = 1

# Get grid dimension (blocks per grid, z dimension)
F grid_dim_z() -> i64 = 1

# Get global thread index (1D linearized)
F global_idx() -> i64 = thread_idx_x() + block_idx_x() * block_dim_x()

# Get global thread index in 2D (x)
F global_idx_x() -> i64 = thread_idx_x() + block_idx_x() * block_dim_x()

# Get global thread index in 2D (y)
F global_idx_y() -> i64 = thread_idx_y() + block_idx_y() * block_dim_y()

# ============================================================================
# Synchronization Functions
# ============================================================================

# Synchronize all threads within a block
# GPU intrinsic: replaced by __syncthreads() (CUDA) / barrier() (OpenCL)
F sync_threads() -> i64 = 0

# Memory fence for global memory
# GPU intrinsic: replaced by __threadfence() (CUDA) / mem_fence() (OpenCL)
F thread_fence() -> i64 = 0

# Memory fence for shared (local) memory
# GPU intrinsic: replaced by __threadfence_block() (CUDA)
F thread_fence_block() -> i64 = 0

# ============================================================================
# Atomic Operations
# ============================================================================

# Atomic add: *addr += val, returns old value
# GPU intrinsic: replaced by atomicAdd() (CUDA) / atomic_add() (OpenCL)
F atomic_add(addr: *i64, val: i64) -> i64 = 0

# Atomic add for f64: *addr += val, returns old value
# STUB: not implemented - returns placeholder value
F atomic_add_f64(addr: *f64, val: f64) -> f64 = 0.0

# Atomic subtract: *addr -= val, returns old value
# STUB: not implemented - returns placeholder value
F atomic_sub(addr: *i64, val: i64) -> i64 = 0

# Atomic minimum: *addr = min(*addr, val), returns old value
# STUB: not implemented - returns placeholder value
F atomic_min(addr: *i64, val: i64) -> i64 = 0

# Atomic maximum: *addr = max(*addr, val), returns old value
# STUB: not implemented - returns placeholder value
F atomic_max(addr: *i64, val: i64) -> i64 = 0

# Atomic AND: *addr &= val, returns old value
# STUB: not implemented - returns placeholder value
F atomic_and(addr: *i64, val: i64) -> i64 = 0

# Atomic OR: *addr |= val, returns old value
# STUB: not implemented - returns placeholder value
F atomic_or(addr: *i64, val: i64) -> i64 = 0

# Atomic XOR: *addr ^= val, returns old value
# STUB: not implemented - returns placeholder value
F atomic_xor(addr: *i64, val: i64) -> i64 = 0

# Atomic compare-and-swap: if *addr == compare then *addr = val
# Returns old value
# STUB: not implemented - returns placeholder value
F atomic_cas(addr: *i64, compare: i64, val: i64) -> i64 = 0

# Atomic exchange: swap *addr with val, returns old value
# STUB: not implemented - returns placeholder value
F atomic_exch(addr: *i64, val: i64) -> i64 = 0

# ============================================================================
# Math Functions (GPU-optimized versions)
# ============================================================================

# Fast square root (may be less precise than CPU version)
F gpu_sqrt(x: f64) -> f64 = sqrt(x)

# Fast reciprocal square root
# GPU intrinsic: replaced by rsqrtf() (CUDA) / native_rsqrt() (OpenCL)
F gpu_rsqrt(x: f64) -> f64 = 1.0 / sqrt(x)

# Fast sine
F gpu_sin(x: f64) -> f64 = sin(x)

# Fast cosine
F gpu_cos(x: f64) -> f64 = cos(x)

# Fast exponential
F gpu_exp(x: f64) -> f64 = exp(x)

# Fast logarithm
F gpu_log(x: f64) -> f64 = log(x)

# Fused multiply-add: a * b + c
# GPU intrinsic: replaced by fma() (CUDA/OpenCL)
F gpu_fma(a: f64, b: f64, c: f64) -> f64 = a * b + c

# ============================================================================
# Shared Memory
# ============================================================================

# Allocate shared memory (per-block, fast access)
# Returns pointer to shared memory region
# Note: size must be known at compile time
F shared_alloc(size: i64) -> i64 = malloc(size)

# ============================================================================
# Memory Operations
# ============================================================================

# Coalesced load from global memory
# GPU intrinsic: direct pointer dereference with coalescing optimization
F gpu_load(addr: *f64) -> f64 = addr[0]

# Coalesced store to global memory
# GPU intrinsic: direct pointer store with coalescing optimization
F gpu_store(addr: *f64, val: f64) -> i64 {
    addr[0] = val
    0
}

# ============================================================================
# Utility Functions
# ============================================================================

# Clamp value to range [lo, hi]
F gpu_clamp(x: f64, lo: f64, hi: f64) -> f64 {
    I x < lo { R lo }
    I x > hi { R hi }
    x
}

# Linear interpolation: a + t * (b - a)
F gpu_lerp(a: f64, b: f64, t: f64) -> f64 = a + t * (b - a)

# Step function: 0.0 if x < edge, else 1.0
F gpu_step(edge: f64, x: f64) -> f64 = x < edge ? 0.0 : 1.0

# Smooth step (Hermite interpolation)
F gpu_smoothstep(edge0: f64, edge1: f64, x: f64) -> f64 {
    t := gpu_clamp((x - edge0) / (edge1 - edge0), 0.0, 1.0)
    t * t * (3.0 - 2.0 * t)
}

# ============================================================================
# Warp/Wavefront Operations (CUDA: warp, AMD: wavefront)
# ============================================================================

# Get lane index within warp (0-31 for NVIDIA, 0-63 for AMD)
F lane_id() -> i64 = thread_idx_x() % 32

# Warp vote: true if all threads have condition true
F warp_all(condition: i64) -> i64 = condition

# Warp vote: true if any thread has condition true
F warp_any(condition: i64) -> i64 = condition

# Warp ballot: returns bitmask of threads with condition true
F warp_ballot(condition: i64) -> i64 = condition

# Warp shuffle: get value from another lane
F warp_shuffle(val: i64, src_lane: i64) -> i64 = val

# Warp shuffle down: get value from lane + delta
F warp_shuffle_down(val: i64, delta: i64) -> i64 = val

# Warp shuffle up: get value from lane - delta
F warp_shuffle_up(val: i64, delta: i64) -> i64 = val

# Warp shuffle xor: get value from lane ^ mask
F warp_shuffle_xor(val: i64, mask: i64) -> i64 = val

# ============================================================================
# Reduction Operations
# ============================================================================

# Block-level sum reduction
# All threads contribute their value, result available to all
F block_reduce_sum(val: f64) -> f64 = val

# Block-level max reduction
F block_reduce_max(val: f64) -> f64 = val

# Block-level min reduction
F block_reduce_min(val: f64) -> f64 = val

# ============================================================================
# Grid Configuration Helpers
# ============================================================================

# Calculate number of blocks needed for n elements
F calc_blocks(n: i64, block_size: i64) -> i64 = (n + block_size - 1) / block_size

# Calculate total threads for n elements
F calc_threads(n: i64, block_size: i64) -> i64 = calc_blocks(n, block_size) * block_size

# ============================================================================
# Metal-specific Functions (Apple GPU)
# ============================================================================

# Threadgroup memory barrier
# STUB: not implemented - returns placeholder value
F threadgroup_barrier() -> i64 = 0  # Replaced by Metal compiler

# Device memory barrier
# STUB: not implemented - returns placeholder value
F device_barrier() -> i64 = 0

# SIMD group (simdgroup) operations
F simd_sum(val: f64) -> f64 = val
F simd_min(val: f64) -> f64 = val
F simd_max(val: f64) -> f64 = val
F simd_broadcast(val: f64, lane: i64) -> f64 = val

# Quad operations (4-wide SIMD)
F quad_sum(val: f64) -> f64 = val
F quad_broadcast(val: f64, lane: i64) -> f64 = val

# ============================================================================
# AVX-512 SIMD Operations (Intel/AMD)
# ============================================================================

# 512-bit vector load/store (16 x f32 or 8 x f64)
# STUB: not implemented - returns placeholder value
F avx512_load_f32(addr: *i64) -> i64 = 0  # Returns vector as i64 handle
# STUB: not implemented - returns placeholder value
F avx512_store_f32(addr: *i64, vec: i64) -> i64 = 0
# STUB: not implemented - returns placeholder value
F avx512_load_f64(addr: *f64) -> i64 = 0
# STUB: not implemented - returns placeholder value
F avx512_store_f64(addr: *f64, vec: i64) -> i64 = 0

# AVX-512 arithmetic
# STUB: not implemented - returns placeholder value
F avx512_add_f32(a: i64, b: i64) -> i64 = 0
# STUB: not implemented - returns placeholder value
F avx512_sub_f32(a: i64, b: i64) -> i64 = 0
# STUB: not implemented - returns placeholder value
F avx512_mul_f32(a: i64, b: i64) -> i64 = 0
# STUB: not implemented - returns placeholder value
F avx512_div_f32(a: i64, b: i64) -> i64 = 0
# STUB: not implemented - returns placeholder value
F avx512_fma_f32(a: i64, b: i64, c: i64) -> i64 = 0

# AVX-512 reduction
# STUB: not implemented - returns placeholder value
F avx512_reduce_add_f32(vec: i64) -> f64 = 0.0
# STUB: not implemented - returns placeholder value
F avx512_reduce_min_f32(vec: i64) -> f64 = 0.0
# STUB: not implemented - returns placeholder value
F avx512_reduce_max_f32(vec: i64) -> f64 = 0.0

# AVX-512 broadcast
# STUB: not implemented - returns placeholder value
F avx512_broadcast_f32(val: f64) -> i64 = 0
# STUB: not implemented - returns placeholder value
F avx512_broadcast_f64(val: f64) -> i64 = 0

# ============================================================================
# AVX2 SIMD Operations (Intel/AMD)
# ============================================================================

# 256-bit vector load/store (8 x f32 or 4 x f64)
# STUB: not implemented - returns placeholder value
F avx2_load_f32(addr: *i64) -> i64 = 0
# STUB: not implemented - returns placeholder value
F avx2_store_f32(addr: *i64, vec: i64) -> i64 = 0
# STUB: not implemented - returns placeholder value
F avx2_load_f64(addr: *f64) -> i64 = 0
# STUB: not implemented - returns placeholder value
F avx2_store_f64(addr: *f64, vec: i64) -> i64 = 0

# AVX2 arithmetic
# STUB: not implemented - returns placeholder value
F avx2_add_f32(a: i64, b: i64) -> i64 = 0
# STUB: not implemented - returns placeholder value
F avx2_sub_f32(a: i64, b: i64) -> i64 = 0
# STUB: not implemented - returns placeholder value
F avx2_mul_f32(a: i64, b: i64) -> i64 = 0
# STUB: not implemented - returns placeholder value
F avx2_fma_f32(a: i64, b: i64, c: i64) -> i64 = 0

# AVX2 broadcast
# STUB: not implemented - returns placeholder value
F avx2_broadcast_f32(val: f64) -> i64 = 0

# ============================================================================
# ARM NEON SIMD Operations
# ============================================================================

# 128-bit vector load/store (4 x f32 or 2 x f64)
# STUB: not implemented - returns placeholder value
F neon_load_f32(addr: *i64) -> i64 = 0
# STUB: not implemented - returns placeholder value
F neon_store_f32(addr: *i64, vec: i64) -> i64 = 0
# STUB: not implemented - returns placeholder value
F neon_load_f64(addr: *f64) -> i64 = 0
# STUB: not implemented - returns placeholder value
F neon_store_f64(addr: *f64, vec: i64) -> i64 = 0

# NEON arithmetic
# STUB: not implemented - returns placeholder value
F neon_add_f32(a: i64, b: i64) -> i64 = 0
# STUB: not implemented - returns placeholder value
F neon_sub_f32(a: i64, b: i64) -> i64 = 0
# STUB: not implemented - returns placeholder value
F neon_mul_f32(a: i64, b: i64) -> i64 = 0
# STUB: not implemented - returns placeholder value
F neon_fma_f32(a: i64, b: i64, c: i64) -> i64 = 0

# NEON reduction
# STUB: not implemented - returns placeholder value
F neon_reduce_add_f32(vec: i64) -> f64 = 0.0
# STUB: not implemented - returns placeholder value
F neon_reduce_min_f32(vec: i64) -> f64 = 0.0
# STUB: not implemented - returns placeholder value
F neon_reduce_max_f32(vec: i64) -> f64 = 0.0

# NEON broadcast
# STUB: not implemented - returns placeholder value
F neon_dup_f32(val: f64) -> i64 = 0

# ============================================================================
# Kernel Launch Configuration
# ============================================================================

# Configure kernel launch parameters
S KernelConfig {
    grid_x: i64,
    grid_y: i64,
    grid_z: i64,
    block_x: i64,
    block_y: i64,
    block_z: i64,
    shared_memory: i64,
}

# Create default kernel configuration
F kernel_config_default() -> KernelConfig = KernelConfig {
    grid_x: 1,
    grid_y: 1,
    grid_z: 1,
    block_x: 256,
    block_y: 1,
    block_z: 1,
    shared_memory: 0,
}

# Create 1D kernel configuration
F kernel_config_1d(n: i64, block_size: i64) -> KernelConfig = KernelConfig {
    grid_x: calc_blocks(n, block_size),
    grid_y: 1,
    grid_z: 1,
    block_x: block_size,
    block_y: 1,
    block_z: 1,
    shared_memory: 0,
}

# Create 2D kernel configuration
F kernel_config_2d(width: i64, height: i64, block_x: i64, block_y: i64) -> KernelConfig = KernelConfig {
    grid_x: calc_blocks(width, block_x),
    grid_y: calc_blocks(height, block_y),
    grid_z: 1,
    block_x: block_x,
    block_y: block_y,
    block_z: 1,
    shared_memory: 0,
}

# ============================================================================
# Host-side GPU Runtime API (linked from gpu_runtime.c)
# These functions manage GPU memory, launch kernels, and query devices.
# Requires: vaisc build --gpu cuda --gpu-compile
# ============================================================================

# --- Memory Management ---

# Allocate GPU device memory (returns device pointer, NULL on failure)
@[extern "C"]
F gpu_alloc(size: i64) -> *i64 { 0 }

# Free GPU device memory
@[extern "C"]
F gpu_free(ptr: *i64) -> i64 { 0 }

# Copy host memory to device
@[extern "C"]
F gpu_memcpy_h2d(dst: *i64, src: *i64, size: i64) -> i64 { 0 }

# Copy device memory to host
@[extern "C"]
F gpu_memcpy_d2h(dst: *i64, src: *i64, size: i64) -> i64 { 0 }

# Copy between device buffers
@[extern "C"]
F gpu_memcpy_d2d(dst: *i64, src: *i64, size: i64) -> i64 { 0 }

# Set device memory to a byte value
@[extern "C"]
F gpu_memset(ptr: *i64, value: i64, size: i64) -> i64 { 0 }

# Allocate unified (managed) memory accessible from host and device
@[extern "C"]
F gpu_alloc_managed(size: i64) -> *i64 { 0 }

# --- Kernel Execution ---

# Launch a CUDA kernel
# kernel_func: function pointer, grid/block x/y/z: launch dimensions
# shared_mem: shared memory bytes, args: argument pointer array, arg_count: number of args
@[extern "C"]
F gpu_launch_kernel(kernel_func: *i64, grid_x: i64, grid_y: i64, grid_z: i64, block_x: i64, block_y: i64, block_z: i64, shared_mem: i64, args: *i64, arg_count: i64) -> i64 { 0 }

# Synchronize device - wait for all pending GPU operations
@[extern "C"]
F gpu_synchronize() -> i64 { 0 }

# --- Stream Management ---

# Create a new CUDA stream (returns stream handle)
@[extern "C"]
F gpu_stream_create() -> *i64 { 0 }

# Destroy a CUDA stream
@[extern "C"]
F gpu_stream_destroy(stream: *i64) -> i64 { 0 }

# Synchronize a specific stream
@[extern "C"]
F gpu_stream_synchronize(stream: *i64) -> i64 { 0 }

# --- Device Management ---

# Get number of CUDA-capable devices
@[extern "C"]
F gpu_device_count() -> i64 { 0 }

# Set the active CUDA device
@[extern "C"]
F gpu_set_device(device_id: i64) -> i64 { 0 }

# Get the current active device ID
@[extern "C"]
F gpu_get_device() -> i64 { 0 }

# Get device name (returns string pointer)
@[extern "C"]
F gpu_device_name(device_id: i64) -> *i8 { 0 }

# Get total device memory in bytes
@[extern "C"]
F gpu_device_total_mem(device_id: i64) -> i64 { 0 }

# Get max threads per block for device
@[extern "C"]
F gpu_device_max_threads(device_id: i64) -> i64 { 0 }

# --- Event Timing ---

# Create a CUDA event (returns event handle)
@[extern "C"]
F gpu_event_create() -> *i64 { 0 }

# Destroy a CUDA event
@[extern "C"]
F gpu_event_destroy(event: *i64) -> i64 { 0 }

# Record an event
@[extern "C"]
F gpu_event_record(event: *i64) -> i64 { 0 }

# Synchronize on an event
@[extern "C"]
F gpu_event_synchronize(event: *i64) -> i64 { 0 }

# Get elapsed time between two events (milliseconds)
@[extern "C"]
F gpu_event_elapsed(start: *i64, end: *i64) -> f64 { 0.0 }

# --- Async Memory Transfer ---

# Asynchronous host-to-device copy on a stream
@[extern "C"]
F gpu_memcpy_h2d_async(dst: *i64, src: *i64, size: i64, stream: *i64) -> i64 { 0 }

# Asynchronous device-to-host copy on a stream
@[extern "C"]
F gpu_memcpy_d2h_async(dst: *i64, src: *i64, size: i64, stream: *i64) -> i64 { 0 }

# --- Unified Memory Hints ---

# Prefetch unified memory to a specific device
@[extern "C"]
F gpu_mem_prefetch(ptr: *i64, size: i64, device_id: i64) -> i64 { 0 }

# Advise runtime about memory access patterns
# advice: 1=ReadMostly, 2=PreferredLocation, 3=AccessedBy
@[extern "C"]
F gpu_mem_advise(ptr: *i64, size: i64, advice: i64, device_id: i64) -> i64 { 0 }

# --- Event-Stream Operations ---

# Record an event on a specific stream
@[extern "C"]
F gpu_event_record_stream(event: *i64, stream: *i64) -> i64 { 0 }

# --- Multi-GPU Peer Access ---

# Enable peer-to-peer access to another device
@[extern "C"]
F gpu_peer_access_enable(peer_device: i64) -> i64 { 0 }

# Disable peer-to-peer access to another device
@[extern "C"]
F gpu_peer_access_disable(peer_device: i64) -> i64 { 0 }

# Check if peer access is possible (returns 1 if yes, 0 if no)
@[extern "C"]
F gpu_peer_can_access(device: i64, peer: i64) -> i64 { 0 }

# Copy memory between devices (peer-to-peer)
@[extern "C"]
F gpu_memcpy_peer(dst: *i64, dst_device: i64, src: *i64, src_device: i64, size: i64) -> i64 { 0 }

# --- Error Handling ---

# Get the last CUDA error code (0 = success)
@[extern "C"]
F gpu_last_error() -> i64 { 0 }

# Get the last CUDA error as string
@[extern "C"]
F gpu_last_error_string() -> *i8 { 0 }

# Reset/clear the last error
@[extern "C"]
F gpu_reset_error() -> i64 { 0 }
