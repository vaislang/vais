# HashMap - hash table with generic keys and values
# Uses separate chaining for collision resolution
# Updated to use Option<T> for error handling
# Supports generic key types via the Hash trait

U std/option
U std/hash

# Helper function to initialize bucket array
F hashmap_init_buckets(buckets: i64, idx: i64, cap: i64) -> i64 {
    I idx >= cap {
        0
    } E {
        store_i64(buckets + idx * 8, 0)
        hashmap_init_buckets(buckets, idx + 1, cap)
    }
}

# Helper function for rehash - copy entries
F hashmap_rehash_bucket(self_ptr: i64, old_buckets: i64, idx: i64, old_cap: i64) -> i64 {
    I idx >= old_cap {
        0
    } E {
        entry_ptr := load_i64(old_buckets + idx * 8)
        hashmap_rehash_entries(self_ptr, entry_ptr)
        hashmap_rehash_bucket(self_ptr, old_buckets, idx + 1, old_cap)
    }
}

# Helper function for rehash - process entry chain
F hashmap_rehash_entries(self_ptr: i64, entry_ptr: i64) -> i64 {
    I entry_ptr == 0 {
        0
    } E {
        key := load_i64(entry_ptr)
        value := load_i64(entry_ptr + 8)
        next := load_i64(entry_ptr + 16)

        # Get self fields
        buckets := load_i64(self_ptr)      # offset 0
        cap := load_i64(self_ptr + 16)     # offset 16

        # Hash and insert using mult_hash for consistency
        h := mult_hash(key)
        hash_idx := h % cap

        # Get current head and insert at head
        old_head := load_i64(buckets + hash_idx * 8)
        new_entry := malloc(24)
        store_i64(new_entry, key)
        store_i64(new_entry + 8, value)
        store_i64(new_entry + 16, old_head)
        store_i64(buckets + hash_idx * 8, new_entry)

        # Update size
        size := load_i64(self_ptr + 8)
        store_i64(self_ptr + 8, size + 1)

        # Free old entry and process next
        free(entry_ptr)
        hashmap_rehash_entries(self_ptr, next)
    }
}

# Helper function for clear - process all buckets
F hashmap_clear_buckets(buckets: i64, idx: i64, cap: i64) -> i64 {
    I idx >= cap {
        0
    } E {
        entry_ptr := load_i64(buckets + idx * 8)
        hashmap_clear_entries(entry_ptr)
        store_i64(buckets + idx * 8, 0)
        hashmap_clear_buckets(buckets, idx + 1, cap)
    }
}

# Helper function for clear - free entry chain
F hashmap_clear_entries(entry_ptr: i64) -> i64 {
    I entry_ptr == 0 {
        0
    } E {
        next := load_i64(entry_ptr + 16)
        free(entry_ptr)
        hashmap_clear_entries(next)
    }
}

# Helper to count total entries in hashmap
F hashmap_count_entries(buckets: i64, idx: i64, cap: i64) -> i64 {
    I idx >= cap {
        0
    } E {
        entry_ptr := load_i64(buckets + idx * 8)
        chain_count := hashmap_count_chain(entry_ptr)
        chain_count + hashmap_count_entries(buckets, idx + 1, cap)
    }
}

# Helper to count entries in a chain
F hashmap_count_chain(entry_ptr: i64) -> i64 {
    I entry_ptr == 0 {
        0
    } E {
        next := load_i64(entry_ptr + 16)
        1 + hashmap_count_chain(next)
    }
}

# Entry node for linked list in each bucket
S Entry<K, V> {
    key: K,         # Key
    value: V,       # Value
    next: i64       # Pointer to next Entry (0 if none)
}

# HashMap structure
S HashMap<K, V> {
    buckets: i64,   # Pointer to array of bucket heads (Entry pointers)
    size: i64,      # Number of key-value pairs
    cap: i64        # Number of buckets
}

X HashMap<K, V> {
    # Create a new HashMap with given capacity
    F with_capacity(capacity: i64) -> HashMap<K, V> {
        cap := I capacity < 8 { 8 } E { capacity }
        # Allocate bucket array (array of pointers, each 8 bytes)
        buckets := malloc(cap * 8)
        # Initialize all buckets to null (0) using recursive helper
        hashmap_init_buckets(buckets, 0, cap)
        HashMap { buckets: buckets, size: 0, cap: cap }
    }

    # Get number of entries
    F len(&self) -> i64 {
        self.size
    }

    # Get capacity (number of buckets)
    F capacity(&self) -> i64 {
        self.cap
    }

    # Check if empty
    F is_empty(&self) -> i64 {
        I self.size == 0 { 1 } E { 0 }
    }

    # Hash function for key
    # Uses the mult_hash function from std/hash for i64 keys
    # For other key types, the key value is treated as i64 (pointer or raw value)
    F hash(&self, key: K) -> i64 {
        # Use the generic mult_hash function for good distribution
        h := mult_hash(key)
        h % self.cap
    }

    # Get value for key, returns 0 if not found
    # Use contains() to check if key exists
    F get(&self, key: K) -> V {
        idx := @.hash(key)
        entry_ptr := load_i64(self.buckets + idx * 8)
        @.get_from_chain(entry_ptr, key)
    }

    # Helper to search through chain
    F get_from_chain(&self, entry_ptr: i64, key: K) -> V {
        I entry_ptr == 0 {
            0
        } E {
            entry_key := load_i64(entry_ptr)
            I entry_key == key {
                load_i64(entry_ptr + 8)
            } E {
                next := load_i64(entry_ptr + 16)
                @.get_from_chain(next, key)
            }
        }
    }

    # Get value for key using Option type
    # Returns Some(value) if found, None if not found
    F get_opt(&self, key: K) -> Option<V> {
        I @.contains(key) == 1 {
            Some(@.get(key))
        } E {
            None
        }
    }

    # Check if key exists
    F contains(&self, key: K) -> i64 {
        idx := @.hash(key)
        entry_ptr := load_i64(self.buckets + idx * 8)
        @.contains_in_chain(entry_ptr, key)
    }

    # Helper to search chain for key
    F contains_in_chain(&self, entry_ptr: i64, key: K) -> i64 {
        I entry_ptr == 0 {
            0
        } E {
            entry_key := load_i64(entry_ptr)
            I entry_key == key {
                1
            } E {
                next := load_i64(entry_ptr + 16)
                @.contains_in_chain(next, key)
            }
        }
    }

    # Set key-value pair, returns previous value or 0 if new
    F set(&self, key: K, value: V) -> V {
        idx := @.hash(key)
        entry_ptr := load_i64(self.buckets + idx * 8)

        # First try to update existing key
        result := @.update_in_chain(entry_ptr, key, value)
        I result >= 0 {
            # Key was found and updated, result is old value
            result
        } E {
            # Key not found, insert new entry at head
            new_entry := malloc(24)  # 3 * 8 bytes for Entry
            store_i64(new_entry, key)        # key
            store_i64(new_entry + 8, value)  # value
            store_i64(new_entry + 16, entry_ptr)  # next = old head
            store_i64(self.buckets + idx * 8, new_entry)  # update bucket head
            self.size = self.size + 1

            # Check if rehash needed (load factor > 0.75)
            I self.size * 4 > self.cap * 3 {
                @.rehash()
            }
            0
        }
    }

    # Helper to update value if key exists in chain
    # Returns old value if found (>= 0), -1 if not found
    F update_in_chain(&self, entry_ptr: i64, key: K, value: V) -> i64 {
        I entry_ptr == 0 {
            0 - 1  # -1 means not found
        } E {
            entry_key := load_i64(entry_ptr)
            I entry_key == key {
                # Key exists, update value
                old_value := load_i64(entry_ptr + 8)
                store_i64(entry_ptr + 8, value)
                old_value
            } E {
                next := load_i64(entry_ptr + 16)
                @.update_in_chain(next, key, value)
            }
        }
    }

    # Remove key-value pair, returns removed value or 0 if not found
    F remove(&self, key: K) -> V {
        idx := @.hash(key)
        entry_ptr := load_i64(self.buckets + idx * 8)
        @.remove_from_chain(idx, 0, entry_ptr, key)
    }

    # Remove key-value pair using Option type
    # Returns Some(value) if found and removed, None if not found
    F remove_opt(&self, key: K) -> Option<V> {
        I @.contains(key) == 1 {
            Some(@.remove(key))
        } E {
            None
        }
    }

    # Helper to remove from chain
    F remove_from_chain(&self, bucket_idx: i64, prev_ptr: i64, entry_ptr: i64, key: K) -> V {
        I entry_ptr == 0 {
            0
        } E {
            entry_key := load_i64(entry_ptr)
            I entry_key == key {
                value := load_i64(entry_ptr + 8)
                next := load_i64(entry_ptr + 16)

                # Use dummy variable to consume if-expression result
                _ := I prev_ptr == 0 {
                    # Removing head of list
                    store_i64(self.buckets + bucket_idx * 8, next)
                    0
                } E {
                    # Removing from middle/end
                    store_i64(prev_ptr + 16, next)
                    0
                }

                free(entry_ptr)
                self.size = self.size - 1
                value
            } E {
                next := load_i64(entry_ptr + 16)
                @.remove_from_chain(bucket_idx, entry_ptr, next, key)
            }
        }
    }

    # Rehash to double capacity
    F rehash(&self) -> i64 {
        old_buckets := self.buckets
        old_cap := self.cap
        new_cap := old_cap * 2

        # Allocate new bucket array
        new_buckets := malloc(new_cap * 8)
        hashmap_init_buckets(new_buckets, 0, new_cap)

        # Update self before rehashing
        self.buckets = new_buckets
        self.cap = new_cap

        # Count entries while rehashing
        # Note: size tracking done manually in rehash helper

        # Reinsert all entries using recursive raw helper
        hashmap_rehash_bucket_raw(new_buckets, new_cap, old_buckets, 0, old_cap)

        # Count total entries (traverse all buckets)
        self.size = hashmap_count_entries(new_buckets, 0, new_cap)

        free(old_buckets)
        1
    }

    # Clear all entries
    F clear(&self) -> i64 {
        hashmap_clear_buckets(self.buckets, 0, self.cap)
        self.size = 0
        0
    }

    # Free all memory
    F drop(&self) -> i64 {
        @.clear()
        free(self.buckets)
        self.buckets = 0
        self.cap = 0
        0
    }
}

# Rehash helper: iterate buckets - takes raw pointer to HashMap
F hashmap_rehash_bucket_raw(m_buckets: i64, m_cap: i64, old_buckets: i64, idx: i64, old_cap: i64) -> i64 {
    I idx >= old_cap {
        0
    } E {
        entry_ptr := load_i64(old_buckets + idx * 8)
        hashmap_rehash_entries_raw(m_buckets, m_cap, entry_ptr)
        hashmap_rehash_bucket_raw(m_buckets, m_cap, old_buckets, idx + 1, old_cap)
    }
}

# Rehash helper: process entry chain - inserts directly using raw pointers
F hashmap_rehash_entries_raw(m_buckets: i64, m_cap: i64, entry_ptr: i64) -> i64 {
    I entry_ptr == 0 {
        0
    } E {
        key := load_i64(entry_ptr)
        value := load_i64(entry_ptr + 8)
        next := load_i64(entry_ptr + 16)

        # Hash the key using mult_hash for consistency
        h := mult_hash(key)
        hash_idx := h % m_cap

        # Insert at head of bucket
        old_head := load_i64(m_buckets + hash_idx * 8)
        new_entry := malloc(24)
        store_i64(new_entry, key)
        store_i64(new_entry + 8, value)
        store_i64(new_entry + 16, old_head)
        store_i64(m_buckets + hash_idx * 8, new_entry)

        # Free old entry and process next
        free(entry_ptr)
        hashmap_rehash_entries_raw(m_buckets, m_cap, next)
    }
}

# Create new HashMap with default capacity (backward compatibility)
F hashmap_new() -> HashMap<i64, i64> {
    HashMap.with_capacity(16)
}
