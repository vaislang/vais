# Vais Self-Hosting Compiler - Pipeline Integration Test
# Tests Lexer -> Parser -> Codegen pipeline
# Standalone test without imports

# ============================================================================
# Helper functions
# ============================================================================

F print_i64(val: i64) -> i64 {
    I val == 0 {
        putchar(48)
        0
    } E I val < 0 {
        putchar(45)
        print_i64(0 - val)
    } E {
        I val >= 10 {
            print_i64(val / 10)
            0
        } E { 0 }
        putchar(48 + (val % 10))
        0
    }
}

# ============================================================================
# Token Constants
# ============================================================================

F TOK_KW_F() -> i64 = 1
F TOK_KW_S() -> i64 = 2
F TOK_KW_E() -> i64 = 3
F TOK_KW_I() -> i64 = 4
F TOK_KW_L() -> i64 = 5
F TOK_KW_R() -> i64 = 13
F TOK_KW_B() -> i64 = 14
F TOK_INT() -> i64 = 51
F TOK_IDENT() -> i64 = 54
F TOK_PLUS() -> i64 = 61
F TOK_MINUS() -> i64 = 62
F TOK_STAR() -> i64 = 63
F TOK_LPAREN() -> i64 = 91
F TOK_RPAREN() -> i64 = 92
F TOK_LBRACE() -> i64 = 93
F TOK_RBRACE() -> i64 = 94
F TOK_COMMA() -> i64 = 101
F TOK_COLON() -> i64 = 102
F TOK_DOT() -> i64 = 104
F TOK_ARROW() -> i64 = 107
F TOK_EQ() -> i64 = 81
F TOK_EOF() -> i64 = 200

# ============================================================================
# Lexer
# ============================================================================

S Lexer {
    source: i64,
    source_len: i64,
    pos: i64,
    line: i64,
    col: i64
}

F is_digit(c: i64) -> i64 {
    I c >= 48 && c <= 57 { 1 } E { 0 }
}

F is_ident_start(c: i64) -> i64 {
    I (c >= 65 && c <= 90) || (c >= 97 && c <= 122) || c == 95 { 1 } E { 0 }
}

F is_ident_char(c: i64) -> i64 {
    I is_ident_start(c) == 1 || is_digit(c) == 1 { 1 } E { 0 }
}

F is_whitespace(c: i64) -> i64 {
    I c == 32 || c == 9 || c == 10 || c == 13 { 1 } E { 0 }
}

X Lexer {
    F new(source: i64, len: i64) -> Lexer = Lexer {
        source: source,
        source_len: len,
        pos: 0,
        line: 1,
        col: 1
    }

    F is_eof(&self) -> i64 {
        I self.pos >= self.source_len { 1 } E { 0 }
    }

    F peek(&self) -> i64 {
        I self.pos >= self.source_len { 0 }
        E { load_byte(self.source + self.pos) }
    }

    F advance(&self) -> i64 {
        I self.pos < self.source_len {
            c := @.peek()
            I c == 10 {
                self.line = self.line + 1
                self.col = 1
                0
            } E {
                self.col = self.col + 1
                0
            }
            self.pos = self.pos + 1
            c
        } E { 0 }
    }

    F skip_whitespace(&self) -> i64 {
        L {
            I @.is_eof() == 1 { B } E { 0 }
            c := @.peek()
            I is_whitespace(c) == 0 { B } E { 0 }
            @.advance()
        }
        1
    }

    F skip_comment(&self) -> i64 {
        L {
            I @.is_eof() == 1 { B } E { 0 }
            I @.peek() == 10 { B } E { 0 }
            @.advance()
        }
        0
    }

    F scan_number(&self, tokens: i64, count: i64) -> i64 {
        start := self.pos
        value: mut i64 = 0
        L {
            I @.is_eof() == 1 { B } E { 0 }
            c := @.peek()
            I is_digit(c) == 0 { B } E { 0 }
            value = value * 10 + (c - 48)
            @.advance()
        }
        # Store token
        ptr := tokens + count * 48
        store_i64(ptr + 0, TOK_INT())
        store_i64(ptr + 8, value)
        store_i64(ptr + 16, 0)
        store_i64(ptr + 24, 0)
        store_i64(ptr + 32, start)
        store_i64(ptr + 40, self.pos)
        1
    }

    F scan_ident(&self, tokens: i64, count: i64) -> i64 {
        start := self.pos
        str_start := self.source + self.pos
        L {
            I @.is_eof() == 1 { B } E { 0 }
            I is_ident_char(@.peek()) == 0 { B } E { 0 }
            @.advance()
        }
        len := self.pos - start

        # Check for single-letter keywords
        kind: mut i64 = TOK_IDENT()
        I len == 1 {
            c := load_byte(str_start)
            I c == 70 { kind = TOK_KW_F(); 0 }
            E I c == 83 { kind = TOK_KW_S(); 0 }
            E I c == 73 { kind = TOK_KW_I(); 0 }
            E I c == 76 { kind = TOK_KW_L(); 0 }
            E I c == 82 { kind = TOK_KW_R(); 0 }
            E I c == 66 { kind = TOK_KW_B(); 0 }
            E I c == 69 { kind = TOK_KW_E(); 0 }
            E { 0 }
        } E { 0 }

        # Store token
        ptr := tokens + count * 48
        store_i64(ptr + 0, kind)
        store_i64(ptr + 8, 0)
        store_i64(ptr + 16, str_start)
        store_i64(ptr + 24, len)
        store_i64(ptr + 32, start)
        store_i64(ptr + 40, self.pos)
        1
    }

    F scan_operator(&self, tokens: i64, count: i64) -> i64 {
        start := self.pos
        c := @.advance()
        kind: mut i64 = 0

        I c == 40 { kind = TOK_LPAREN(); 0 }
        E I c == 41 { kind = TOK_RPAREN(); 0 }
        E I c == 123 { kind = TOK_LBRACE(); 0 }
        E I c == 125 { kind = TOK_RBRACE(); 0 }
        E I c == 44 { kind = TOK_COMMA(); 0 }
        E I c == 58 { kind = TOK_COLON(); 0 }
        E I c == 46 { kind = TOK_DOT(); 0 }
        E I c == 43 { kind = TOK_PLUS(); 0 }
        E I c == 45 {
            I @.peek() == 62 {
                @.advance()
                kind = TOK_ARROW()
                0
            } E {
                kind = TOK_MINUS()
                0
            }
        }
        E I c == 42 { kind = TOK_STAR(); 0 }
        E I c == 61 { kind = TOK_EQ(); 0 }
        E { 0 }

        # Store token
        ptr := tokens + count * 48
        store_i64(ptr + 0, kind)
        store_i64(ptr + 8, 0)
        store_i64(ptr + 16, 0)
        store_i64(ptr + 24, 0)
        store_i64(ptr + 32, start)
        store_i64(ptr + 40, self.pos)
        1
    }

    F tokenize(&self, tokens: i64) -> i64 {
        count: mut i64 = 0
        L {
            @.skip_whitespace()

            I @.is_eof() == 1 { B } E { 0 }

            c := @.peek()

            # Skip comments
            I c == 35 {
                @.skip_comment()
            } E I is_digit(c) == 1 {
                @.scan_number(tokens, count)
                count = count + 1
                0
            } E I is_ident_start(c) == 1 {
                @.scan_ident(tokens, count)
                count = count + 1
                0
            } E {
                @.scan_operator(tokens, count)
                count = count + 1
                0
            }
        }

        # Add EOF token
        ptr := tokens + count * 48
        store_i64(ptr + 0, TOK_EOF())
        store_i64(ptr + 32, self.pos)
        store_i64(ptr + 40, self.pos)
        count + 1
    }
}

# ============================================================================
# Main Test
# ============================================================================

F main() -> i64 {
    puts("=== Vais Pipeline Test ===\n")

    # Test source: "F add(a: i64) -> i64 = a + 1"
    source := malloc(32)
    store_byte(source + 0, 70)   # F
    store_byte(source + 1, 32)   # space
    store_byte(source + 2, 97)   # a
    store_byte(source + 3, 100)  # d
    store_byte(source + 4, 100)  # d
    store_byte(source + 5, 40)   # (
    store_byte(source + 6, 97)   # a
    store_byte(source + 7, 58)   # :
    store_byte(source + 8, 32)   # space
    store_byte(source + 9, 105)  # i
    store_byte(source + 10, 54)  # 6
    store_byte(source + 11, 52)  # 4
    store_byte(source + 12, 41)  # )
    store_byte(source + 13, 32)  # space
    store_byte(source + 14, 45)  # -
    store_byte(source + 15, 62)  # >
    store_byte(source + 16, 32)  # space
    store_byte(source + 17, 105) # i
    store_byte(source + 18, 54)  # 6
    store_byte(source + 19, 52)  # 4
    store_byte(source + 20, 32)  # space
    store_byte(source + 21, 61)  # =
    store_byte(source + 22, 32)  # space
    store_byte(source + 23, 97)  # a
    store_byte(source + 24, 32)  # space
    store_byte(source + 25, 43)  # +
    store_byte(source + 26, 32)  # space
    store_byte(source + 27, 49)  # 1
    source_len := 28

    puts("Source: F add(a: i64) -> i64 = a + 1\n\n")

    # Create lexer and tokenize
    lexer := Lexer.new(source, source_len)
    tokens := malloc(64 * 48)  # 64 tokens max, 48 bytes each
    token_count := lexer.tokenize(tokens)

    puts("Tokens found: ")
    print_i64(token_count)
    putchar(10)

    # Print tokens
    i: mut i64 = 0
    L {
        I i >= token_count { B } E { 0 }
        ptr := tokens + i * 48
        kind := load_i64(ptr + 0)
        value := load_i64(ptr + 8)
        str_ptr := load_i64(ptr + 16)
        str_len := load_i64(ptr + 24)

        puts("  Token ")
        print_i64(i)
        puts(": kind=")
        print_i64(kind)

        I kind == TOK_INT() {
            puts(" value=")
            print_i64(value)
            0
        } E I str_len > 0 {
            puts(" str='")
            j: mut i64 = 0
            L {
                I j >= str_len { B } E { 0 }
                putchar(load_byte(str_ptr + j))
                j = j + 1
            }
            puts("'")
            0
        } E { 0 }

        putchar(10)
        i = i + 1
    }

    # Verify expected tokens
    puts("\n--- Verification ---\n")
    pass: mut i64 = 1

    # Token 0: F
    tok0_kind := load_i64(tokens + 0 * 48 + 0)
    I tok0_kind == TOK_KW_F() {
        puts("[PASS] Token 0 is F keyword\n")
        0
    } E {
        puts("[FAIL] Token 0 is not F keyword\n")
        pass = 0
        0
    }

    # Token 1: add
    tok1_kind := load_i64(tokens + 1 * 48 + 0)
    tok1_len := load_i64(tokens + 1 * 48 + 24)
    I tok1_kind == TOK_IDENT() && tok1_len == 3 {
        puts("[PASS] Token 1 is identifier 'add'\n")
        0
    } E {
        puts("[FAIL] Token 1 is not identifier 'add'\n")
        pass = 0
        0
    }

    # Token 2: (
    tok2_kind := load_i64(tokens + 2 * 48 + 0)
    I tok2_kind == TOK_LPAREN() {
        puts("[PASS] Token 2 is (\n")
        0
    } E {
        puts("[FAIL] Token 2 is not (\n")
        pass = 0
        0
    }

    # Token 11: +
    tok11_kind := load_i64(tokens + 11 * 48 + 0)
    I tok11_kind == TOK_PLUS() {
        puts("[PASS] Token 11 is +\n")
        0
    } E {
        puts("[FAIL] Token 11 is not +\n")
        pass = 0
        0
    }

    # Token 12: 1
    tok12_kind := load_i64(tokens + 12 * 48 + 0)
    tok12_value := load_i64(tokens + 12 * 48 + 8)
    I tok12_kind == TOK_INT() && tok12_value == 1 {
        puts("[PASS] Token 12 is integer 1\n")
        0
    } E {
        puts("[FAIL] Token 12 is not integer 1\n")
        pass = 0
        0
    }

    # Last token: EOF
    last_idx := token_count - 1
    tok_eof_kind := load_i64(tokens + last_idx * 48 + 0)
    I tok_eof_kind == TOK_EOF() {
        puts("[PASS] Last token is EOF\n")
        0
    } E {
        puts("[FAIL] Last token is not EOF\n")
        pass = 0
        0
    }

    free(tokens)
    free(source)

    I pass == 1 {
        puts("\n=== All Pipeline Tests PASSED ===\n")
        0
    } E {
        puts("\n=== Some Pipeline Tests FAILED ===\n")
        1
    }
}
