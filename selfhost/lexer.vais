# Vais Self-Hosting Compiler - Lexer Module
# Tokenizer for Vais source code
# Includes token definitions (merged to avoid import issues)

# ===== Token Definitions =====

# Token type constants (using i64 since Vais enum support is limited)
# We use integer constants to represent token types

# Keyword tokens (1-30)
F TOK_KW_F() -> i64 = 1        # function
F TOK_KW_S() -> i64 = 2        # struct
F TOK_KW_E() -> i64 = 3        # enum
F TOK_KW_I() -> i64 = 4        # if
F TOK_KW_L() -> i64 = 5        # loop
F TOK_KW_M() -> i64 = 6        # match
F TOK_KW_W() -> i64 = 7        # trait (With)
F TOK_KW_X() -> i64 = 8        # impl (eXtend)
F TOK_KW_T() -> i64 = 9        # type
F TOK_KW_U() -> i64 = 10       # use
F TOK_KW_P() -> i64 = 11       # pub
F TOK_KW_A() -> i64 = 12       # async
F TOK_KW_R() -> i64 = 13       # return
F TOK_KW_B() -> i64 = 14       # break
F TOK_KW_C() -> i64 = 15       # continue
F TOK_KW_TRUE() -> i64 = 16
F TOK_KW_FALSE() -> i64 = 17
F TOK_KW_MUT() -> i64 = 18
F TOK_KW_ELSE() -> i64 = 19

# Type keyword tokens (31-50)
F TOK_TY_I8() -> i64 = 31
F TOK_TY_I16() -> i64 = 32
F TOK_TY_I32() -> i64 = 33
F TOK_TY_I64() -> i64 = 34
F TOK_TY_I128() -> i64 = 35
F TOK_TY_U8() -> i64 = 36
F TOK_TY_U16() -> i64 = 37
F TOK_TY_U32() -> i64 = 38
F TOK_TY_U64() -> i64 = 39
F TOK_TY_U128() -> i64 = 40
F TOK_TY_F32() -> i64 = 41
F TOK_TY_F64() -> i64 = 42
F TOK_TY_BOOL() -> i64 = 43
F TOK_TY_STR() -> i64 = 44

# Literal tokens (51-60)
F TOK_INT() -> i64 = 51        # Integer literal
F TOK_FLOAT() -> i64 = 52      # Float literal
F TOK_STRING() -> i64 = 53     # String literal
F TOK_IDENT() -> i64 = 54      # Identifier

# Operator tokens (61-100)
F TOK_PLUS() -> i64 = 61       # +
F TOK_MINUS() -> i64 = 62      # -
F TOK_STAR() -> i64 = 63       # *
F TOK_SLASH() -> i64 = 64      # /
F TOK_PERCENT() -> i64 = 65    # %
F TOK_LT() -> i64 = 66         # <
F TOK_GT() -> i64 = 67         # >
F TOK_LT_EQ() -> i64 = 68      # <=
F TOK_GT_EQ() -> i64 = 69      # >=
F TOK_EQ_EQ() -> i64 = 70      # ==
F TOK_NOT_EQ() -> i64 = 71     # !=
F TOK_AMP() -> i64 = 72        # &
F TOK_PIPE() -> i64 = 73       # |
F TOK_CARET() -> i64 = 74      # ^
F TOK_TILDE() -> i64 = 75      # ~
F TOK_SHL() -> i64 = 76        # <<
F TOK_SHR() -> i64 = 77        # >>
F TOK_BANG() -> i64 = 78       # !
F TOK_AND() -> i64 = 79        # &&
F TOK_OR() -> i64 = 80         # ||

# Assignment tokens (81-90)
F TOK_EQ() -> i64 = 81         # =
F TOK_COLON_EQ() -> i64 = 82   # :=
F TOK_PLUS_EQ() -> i64 = 83    # +=
F TOK_MINUS_EQ() -> i64 = 84   # -=
F TOK_STAR_EQ() -> i64 = 85    # *=
F TOK_SLASH_EQ() -> i64 = 86   # /=

# Delimiter tokens (91-100)
F TOK_LPAREN() -> i64 = 91     # (
F TOK_RPAREN() -> i64 = 92     # )
F TOK_LBRACE() -> i64 = 93     # {
F TOK_RBRACE() -> i64 = 94     # }
F TOK_LBRACKET() -> i64 = 95   # [
F TOK_RBRACKET() -> i64 = 96   # ]

# Punctuation tokens (101-120)
F TOK_COMMA() -> i64 = 101     # ,
F TOK_COLON() -> i64 = 102     # :
F TOK_SEMI() -> i64 = 103      # ;
F TOK_DOT() -> i64 = 104       # .
F TOK_DOT_DOT() -> i64 = 105   # ..
F TOK_DOT_DOT_EQ() -> i64 = 106 # ..=
F TOK_ARROW() -> i64 = 107     # ->
F TOK_FAT_ARROW() -> i64 = 108 # =>
F TOK_COLON_COLON() -> i64 = 109 # ::
F TOK_QUESTION() -> i64 = 110  # ?
F TOK_AT() -> i64 = 111        # @
F TOK_HASH() -> i64 = 112      # #

# Special tokens
F TOK_EOF() -> i64 = 200
F TOK_ERROR() -> i64 = 201

# Token structure - stores token type and associated data
S Token {
    kind: i64,           # Token type constant
    value: i64,          # For integer/float literals
    str_ptr: i64,        # Pointer to string data
    str_len: i64,        # Length of string data
    span_start: i64,     # Source start position
    span_end: i64        # Source end position
}

X Token {
    # Create a simple token (no value)
    F simple(kind: i64, start: i64, end: i64) -> Token = Token {
        kind: kind,
        value: 0,
        str_ptr: 0,
        str_len: 0,
        span_start: start,
        span_end: end
    }

    # Create an integer literal token
    F int_lit(val: i64, start: i64, end: i64) -> Token = Token {
        kind: TOK_INT(),
        value: val,
        str_ptr: 0,
        str_len: 0,
        span_start: start,
        span_end: end
    }

    # Create a float literal token
    F float_lit(val: i64, start: i64, end: i64) -> Token = Token {
        kind: TOK_FLOAT(),
        value: val,
        str_ptr: 0,
        str_len: 0,
        span_start: start,
        span_end: end
    }

    # Create a string/identifier token
    F string_token(kind: i64, ptr: i64, len: i64, start: i64, end: i64) -> Token = Token {
        kind: kind,
        value: 0,
        str_ptr: ptr,
        str_len: len,
        span_start: start,
        span_end: end
    }
}

# Token list for lexer output
S TokenList {
    data: i64,    # Pointer to Token array
    len: i64,     # Current number of tokens
    cap: i64      # Capacity
}

X TokenList {
    # Create a new token list
    F new(capacity: i64) -> TokenList {
        data := malloc(capacity * 48)
        TokenList { data: data, len: 0, cap: capacity }
    }

    # Add a token to the list (pass fields directly to avoid struct parameter bug)
    F push_fields(&self, kind: i64, value: i64, str_ptr: i64, str_len: i64, span_start: i64, span_end: i64) -> i64 {
        I self.len >= self.cap {
            @.grow()
            0
        } E {
            0
        }
        ptr := self.data + self.len * 48
        store_token_fields(ptr, kind, value, str_ptr, str_len, span_start, span_end)
        self.len = self.len + 1
        self.len
    }

    # Get token at index
    F get(&self, index: i64) -> Token {
        ptr := self.data + index * 48
        Token {
            kind: load_i64(ptr + 0),
            value: load_i64(ptr + 8),
            str_ptr: load_i64(ptr + 16),
            str_len: load_i64(ptr + 24),
            span_start: load_i64(ptr + 32),
            span_end: load_i64(ptr + 40)
        }
    }

    # Grow the list using 64-bit copies
    F grow(&self) -> i64 {
        new_cap := self.cap * 2
        new_data := malloc(new_cap * 48)
        # Copy data in 8-byte chunks (each token is 48 bytes = 6 i64 fields)
        src := self.data
        dst := new_data
        total_bytes := self.len * 48
        i: mut i64 = 0
        L {
            I i >= total_bytes { B }
            v := load_i64(src + i)
            store_i64(dst + i, v)
            i = i + 8
        }
        old_data := self.data
        self.data = new_data
        self.cap = new_cap
        # Note: leaking old memory for now - will fix after compiler supports void
        1
    }

    # Get length
    F len(&self) -> i64 = self.len

    # Free the list
    F drop(&self) -> i64 {
        # Note: leaking memory for now - will fix after compiler supports void
        1
    }
}

# store_token_fields: store token fields to dst pointer
F store_token_fields(dst: i64, kind: i64, value: i64, str_ptr: i64, str_len: i64, span_start: i64, span_end: i64) -> i64 {
    store_i64(dst + 0, kind)
    store_i64(dst + 8, value)
    store_i64(dst + 16, str_ptr)
    store_i64(dst + 24, str_len)
    store_i64(dst + 32, span_start)
    store_i64(dst + 40, span_end)
    1
}

# ===== Lexer Helper Functions =====

# Check if character is digit (0-9: ASCII 48-57)
F is_digit(c: i64) -> i64 {
    I c >= 48 && c <= 57 { 1 } E { 0 }
}

# Check if character can start identifier (A-Z: 65-90, a-z: 97-122, _: 95)
F is_ident_start(c: i64) -> i64 {
    I (c >= 65 && c <= 90) || (c >= 97 && c <= 122) || c == 95 {
        1
    } E {
        0
    }
}

# Check if character can be part of identifier
F is_ident_char(c: i64) -> i64 {
    I is_ident_start(c) == 1 || is_digit(c) == 1 { 1 } E { 0 }
}

# Check if character is whitespace (space:32, tab:9, newline:10, carriage return:13)
F is_whitespace(c: i64) -> i64 {
    I c == 32 || c == 9 || c == 10 || c == 13 { 1 } E { 0 }
}

# Check if character is hex digit (0-9, A-F, a-f)
F is_hex_digit(c: i64) -> i64 {
    I is_digit(c) == 1 || (c >= 65 && c <= 70) || (c >= 97 && c <= 102) { 1 } E { 0 }
}

# Convert hex digit to value
F hex_digit_value(c: i64) -> i64 {
    I c >= 48 && c <= 57 {
        c - 48
    } E I c >= 65 && c <= 70 {
        c - 55
    } E I c >= 97 && c <= 102 {
        c - 87
    } E {
        0
    }
}

# ===== Lexer State =====

S Lexer {
    source: i64,     # Pointer to source string
    source_len: i64, # Length of source
    pos: i64,        # Current position
    line: i64,       # Current line number
    col: i64         # Current column number
}

X Lexer {
    # Create a new lexer for the given source code
    F new(source: i64, len: i64) -> Lexer = Lexer {
        source: source,
        source_len: len,
        pos: 0,
        line: 1,
        col: 1
    }

    # Check if at end of source
    F is_eof(&self) -> i64 {
        I self.pos >= self.source_len { 1 } E { 0 }
    }

    # Peek current character (returns 0 if at end)
    F peek(&self) -> i64 {
        I self.pos >= self.source_len {
            0
        } E {
            load_byte(self.source + self.pos)
        }
    }

    # Peek next character
    F peek_next(&self) -> i64 {
        I self.pos + 1 >= self.source_len {
            0
        } E {
            load_byte(self.source + self.pos + 1)
        }
    }

    # Advance to next character and return it
    F advance(&self) -> i64 {
        I self.pos < self.source_len {
            c := @.peek()
            I c == 10 {
                self.line = self.line + 1
                self.col = 1
                0
            } E {
                self.col = self.col + 1
                0
            }
            self.pos = self.pos + 1
            c
        } E {
            0
        }
    }

    # Skip whitespace and comments
    F skip_whitespace(&self) -> i64 {
        L {
            I @.is_eof() == 1 { B }
            c := @.peek()

            I is_whitespace(c) == 1 {
                @.advance()
                0
            } E I c == 35 {
                # '#' starts a comment - skip until end of line
                L {
                    I @.is_eof() == 1 { B }
                    I @.peek() == 10 { B }
                    @.advance()
                }
                0
            } E {
                R 1
            }
        }
        1
    }

    # Scan a number literal (integer or float)
    F scan_number(&self) -> Token {
        start := self.pos
        value: mut i64 = 0
        is_float: mut i64 = 0

        # Check for hex literal (0x or 0X)
        I @.peek() == 48 && (@.peek_next() == 120 || @.peek_next() == 88) {
            @.advance()
            @.advance()
            L {
                I @.is_eof() == 1 { B }
                c := @.peek()
                I is_hex_digit(c) == 0 { B }
                value = value * 16 + hex_digit_value(c)
                @.advance()
            }
            R Token.int_lit(value, start, self.pos)
        }

        # Regular decimal number
        L {
            I @.is_eof() == 1 { B }
            c := @.peek()
            I is_digit(c) == 0 { B }
            value = value * 10 + (c - 48)
            @.advance()
        }

        # Check for float (decimal point)
        I @.peek() == 46 && is_digit(@.peek_next()) == 1 {
            is_float = 1
            @.advance()
            L {
                I @.is_eof() == 1 { B }
                I is_digit(@.peek()) == 0 { B }
                @.advance()
            }
            0
        } E {
            0
        }

        # Check for exponent
        I @.peek() == 101 || @.peek() == 69 {
            is_float = 1
            @.advance()
            I @.peek() == 43 || @.peek() == 45 {
                @.advance()
                0
            } E {
                0
            }
            L {
                I @.is_eof() == 1 { B }
                I is_digit(@.peek()) == 0 { B }
                @.advance()
            }
            0
        } E {
            0
        }

        I is_float == 1 {
            Token.float_lit(value, start, self.pos)
        } E {
            Token.int_lit(value, start, self.pos)
        }
    }

    # Scan an identifier or keyword
    F scan_ident(&self) -> Token {
        start := self.pos
        str_start := self.source + self.pos

        L {
            I @.is_eof() == 1 { B }
            I is_ident_char(@.peek()) == 0 { B }
            @.advance()
        }

        len := self.pos - start

        # Check for single-letter keywords
        I len == 1 {
            c := load_byte(str_start)
            I c == 70 { R Token.simple(TOK_KW_F(), start, self.pos) }
            I c == 83 { R Token.simple(TOK_KW_S(), start, self.pos) }
            I c == 69 { R Token.simple(TOK_KW_E(), start, self.pos) }
            I c == 73 { R Token.simple(TOK_KW_I(), start, self.pos) }
            I c == 76 { R Token.simple(TOK_KW_L(), start, self.pos) }
            I c == 77 { R Token.simple(TOK_KW_M(), start, self.pos) }
            I c == 87 { R Token.simple(TOK_KW_W(), start, self.pos) }
            I c == 88 { R Token.simple(TOK_KW_X(), start, self.pos) }
            I c == 84 { R Token.simple(TOK_KW_T(), start, self.pos) }
            I c == 85 { R Token.simple(TOK_KW_U(), start, self.pos) }
            I c == 80 { R Token.simple(TOK_KW_P(), start, self.pos) }
            I c == 65 { R Token.simple(TOK_KW_A(), start, self.pos) }
            I c == 82 { R Token.simple(TOK_KW_R(), start, self.pos) }
            I c == 66 { R Token.simple(TOK_KW_B(), start, self.pos) }
            I c == 67 { R Token.simple(TOK_KW_C(), start, self.pos) }
            0
        } E {
            0
        }

        # Check for type keywords (2-5 characters)
        I len == 2 {
            c0 := load_byte(str_start)
            c1 := load_byte(str_start + 1)
            I c0 == 105 && c1 == 56 { R Token.simple(TOK_TY_I8(), start, self.pos) }
            I c0 == 117 && c1 == 56 { R Token.simple(TOK_TY_U8(), start, self.pos) }
            0
        } E {
            0
        }

        I len == 3 {
            c0 := load_byte(str_start)
            c1 := load_byte(str_start + 1)
            c2 := load_byte(str_start + 2)
            I c0 == 105 && c1 == 54 && c2 == 52 { R Token.simple(TOK_TY_I64(), start, self.pos) }
            I c0 == 105 && c1 == 51 && c2 == 50 { R Token.simple(TOK_TY_I32(), start, self.pos) }
            I c0 == 105 && c1 == 49 && c2 == 54 { R Token.simple(TOK_TY_I16(), start, self.pos) }
            I c0 == 117 && c1 == 54 && c2 == 52 { R Token.simple(TOK_TY_U64(), start, self.pos) }
            I c0 == 117 && c1 == 51 && c2 == 50 { R Token.simple(TOK_TY_U32(), start, self.pos) }
            I c0 == 117 && c1 == 49 && c2 == 54 { R Token.simple(TOK_TY_U16(), start, self.pos) }
            I c0 == 102 && c1 == 54 && c2 == 52 { R Token.simple(TOK_TY_F64(), start, self.pos) }
            I c0 == 102 && c1 == 51 && c2 == 50 { R Token.simple(TOK_TY_F32(), start, self.pos) }
            I c0 == 115 && c1 == 116 && c2 == 114 { R Token.simple(TOK_TY_STR(), start, self.pos) }
            I c0 == 109 && c1 == 117 && c2 == 116 { R Token.simple(TOK_KW_MUT(), start, self.pos) }
            0
        } E {
            0
        }

        I len == 4 {
            c0 := load_byte(str_start)
            c1 := load_byte(str_start + 1)
            c2 := load_byte(str_start + 2)
            c3 := load_byte(str_start + 3)
            I c0 == 116 && c1 == 114 && c2 == 117 && c3 == 101 { R Token.simple(TOK_KW_TRUE(), start, self.pos) }
            I c0 == 98 && c1 == 111 && c2 == 111 && c3 == 108 { R Token.simple(TOK_TY_BOOL(), start, self.pos) }
            I c0 == 105 && c1 == 49 && c2 == 50 && c3 == 56 { R Token.simple(TOK_TY_I128(), start, self.pos) }
            I c0 == 117 && c1 == 49 && c2 == 50 && c3 == 56 { R Token.simple(TOK_TY_U128(), start, self.pos) }
            I c0 == 101 && c1 == 108 && c2 == 115 && c3 == 101 { R Token.simple(TOK_KW_ELSE(), start, self.pos) }
            0
        } E {
            0
        }

        I len == 5 {
            c0 := load_byte(str_start)
            c1 := load_byte(str_start + 1)
            c2 := load_byte(str_start + 2)
            c3 := load_byte(str_start + 3)
            c4 := load_byte(str_start + 4)
            I c0 == 102 && c1 == 97 && c2 == 108 && c3 == 115 && c4 == 101 { R Token.simple(TOK_KW_FALSE(), start, self.pos) }
            0
        } E {
            0
        }

        # Not a keyword, return as identifier
        Token.string_token(TOK_IDENT(), str_start, len, start, self.pos)
    }

    # Scan a string literal
    F scan_string(&self) -> Token {
        start := self.pos
        @.advance()
        str_start := self.source + self.pos

        L {
            I @.is_eof() == 1 { B }
            c := @.peek()
            I c == 34 { B }
            I c == 92 {
                @.advance()
                I @.is_eof() == 0 {
                    @.advance()
                    0
                } E {
                    0
                }
            } E {
                @.advance()
                0
            }
        }

        len := self.source + self.pos - str_start

        I @.is_eof() == 0 && @.peek() == 34 {
            @.advance()
            0
        } E {
            0
        }

        Token.string_token(TOK_STRING(), str_start, len, start, self.pos)
    }

    # Scan an operator or punctuation
    F scan_operator(&self) -> Token {
        start := self.pos
        c := @.advance()

        # Single-character tokens
        I c == 40 { R Token.simple(TOK_LPAREN(), start, self.pos) }
        I c == 41 { R Token.simple(TOK_RPAREN(), start, self.pos) }
        I c == 123 { R Token.simple(TOK_LBRACE(), start, self.pos) }
        I c == 125 { R Token.simple(TOK_RBRACE(), start, self.pos) }
        I c == 91 { R Token.simple(TOK_LBRACKET(), start, self.pos) }
        I c == 93 { R Token.simple(TOK_RBRACKET(), start, self.pos) }
        I c == 44 { R Token.simple(TOK_COMMA(), start, self.pos) }
        I c == 59 { R Token.simple(TOK_SEMI(), start, self.pos) }
        I c == 63 { R Token.simple(TOK_QUESTION(), start, self.pos) }
        I c == 64 { R Token.simple(TOK_AT(), start, self.pos) }
        I c == 126 { R Token.simple(TOK_TILDE(), start, self.pos) }
        I c == 37 { R Token.simple(TOK_PERCENT(), start, self.pos) }
        I c == 94 { R Token.simple(TOK_CARET(), start, self.pos) }

        # Two-character operators
        I c == 43 {
            I @.peek() == 61 {
                @.advance()
                R Token.simple(TOK_PLUS_EQ(), start, self.pos)
            }
            R Token.simple(TOK_PLUS(), start, self.pos)
        }
        I c == 45 {
            I @.peek() == 62 {
                @.advance()
                R Token.simple(TOK_ARROW(), start, self.pos)
            }
            I @.peek() == 61 {
                @.advance()
                R Token.simple(TOK_MINUS_EQ(), start, self.pos)
            }
            R Token.simple(TOK_MINUS(), start, self.pos)
        }
        I c == 42 {
            I @.peek() == 61 {
                @.advance()
                R Token.simple(TOK_STAR_EQ(), start, self.pos)
            }
            R Token.simple(TOK_STAR(), start, self.pos)
        }
        I c == 47 {
            I @.peek() == 61 {
                @.advance()
                R Token.simple(TOK_SLASH_EQ(), start, self.pos)
            }
            R Token.simple(TOK_SLASH(), start, self.pos)
        }
        I c == 60 {
            I @.peek() == 61 {
                @.advance()
                R Token.simple(TOK_LT_EQ(), start, self.pos)
            }
            I @.peek() == 60 {
                @.advance()
                R Token.simple(TOK_SHL(), start, self.pos)
            }
            R Token.simple(TOK_LT(), start, self.pos)
        }
        I c == 62 {
            I @.peek() == 61 {
                @.advance()
                R Token.simple(TOK_GT_EQ(), start, self.pos)
            }
            I @.peek() == 62 {
                @.advance()
                R Token.simple(TOK_SHR(), start, self.pos)
            }
            R Token.simple(TOK_GT(), start, self.pos)
        }
        I c == 61 {
            I @.peek() == 61 {
                @.advance()
                R Token.simple(TOK_EQ_EQ(), start, self.pos)
            }
            I @.peek() == 62 {
                @.advance()
                R Token.simple(TOK_FAT_ARROW(), start, self.pos)
            }
            R Token.simple(TOK_EQ(), start, self.pos)
        }
        I c == 33 {
            I @.peek() == 61 {
                @.advance()
                R Token.simple(TOK_NOT_EQ(), start, self.pos)
            }
            R Token.simple(TOK_BANG(), start, self.pos)
        }
        I c == 38 {
            I @.peek() == 38 {
                @.advance()
                R Token.simple(TOK_AND(), start, self.pos)
            }
            R Token.simple(TOK_AMP(), start, self.pos)
        }
        I c == 124 {
            I @.peek() == 124 {
                @.advance()
                R Token.simple(TOK_OR(), start, self.pos)
            }
            R Token.simple(TOK_PIPE(), start, self.pos)
        }
        I c == 58 {
            I @.peek() == 61 {
                @.advance()
                R Token.simple(TOK_COLON_EQ(), start, self.pos)
            }
            I @.peek() == 58 {
                @.advance()
                R Token.simple(TOK_COLON_COLON(), start, self.pos)
            }
            R Token.simple(TOK_COLON(), start, self.pos)
        }
        I c == 46 {
            I @.peek() == 46 {
                @.advance()
                I @.peek() == 61 {
                    @.advance()
                    R Token.simple(TOK_DOT_DOT_EQ(), start, self.pos)
                }
                R Token.simple(TOK_DOT_DOT(), start, self.pos)
            }
            R Token.simple(TOK_DOT(), start, self.pos)
        }

        Token.simple(TOK_ERROR(), start, self.pos)
    }

    # Get the next token
    F next_token(&self) -> Token {
        @.skip_whitespace()

        I @.is_eof() == 1 {
            R Token.simple(TOK_EOF(), self.pos, self.pos)
        }

        c := @.peek()

        I is_digit(c) == 1 {
            R @.scan_number()
        }

        I is_ident_start(c) == 1 {
            R @.scan_ident()
        }

        I c == 34 {
            R @.scan_string()
        }

        @.scan_operator()
    }

    # Tokenize the entire source and return a TokenList
    F tokenize(&self) -> TokenList {
        tokens := TokenList.new(256)

        L {
            tok := @.next_token()
            tokens.push_fields(tok.kind, tok.value, tok.str_ptr, tok.str_len, tok.span_start, tok.span_end)
            I tok.kind == TOK_EOF() { B }
        }

        tokens
    }
}

# Test function (renamed from main to allow module import)
F test_lexer_main() -> i64 {
    puts("Lexer test")
    0
}
